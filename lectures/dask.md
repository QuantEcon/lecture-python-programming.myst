---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

(dask)=
```{raw} jupyter
<div id="qe-notebook-header" align="right" style="text-align:right;">
        <a href="https://quantecon.org/" title="quantecon.org">
                <img style="width:250px;display:inline;" width="250px" src="https://assets.quantecon.org/img/qe-menubar-logo.svg" alt="QuantEcon">
        </a>
</div>
```


# Dask

## Overview

In this lecture, we will continue our exploration on parallelization in Python.

Modern datasets are in the size of trillion, which requires more computing power to speed operations on enomours datasets.

We would want to harvest the processing power in our local machines and potentially clusters in data processing.

We mentioned that multiprocessing is different from multithreading their treatment of memory space. 

In multiprocessing, each process has its own memory space and communicate data through channels between processes.

The overheads and communication cost is higher for multiprocessing compared to multithreading, which is lightweight and easier to start.

But, it brings several desirable advantages:

- Multiprocessing is another way to bypass GIL by employing subprocesses instead of threads.
- Multiprocessing is more flexible and can be distributed across clusters.
- It can speed up operations that are CPU-bound, which cannot be accelerated using multithreading.

Dask is a popular Python library that allows users to speed up operatios conveniently in high-level implementations and control the low level APIs such as Delayed and Future to parallelize customise algorithms.

This lecture will focus on [Dask](https://docs.dask.org/en/stable/why.html#why-dask) because

- It builds on the familiar command in NumPy and Pandas.
- It can be deployed on a single machine and easily scaled to multiple clusters.
- It will carefully manage the memory to enable operations that are larger than the memory of the machine.

In this lecture, we will explore benefits that Dask brings. 

We will start with the general framework, go through high-level APIs of Dask, and then dig into the lower level APIs.

## Dask Clusters

Dask clusters gives Dask the power to leverage the power of multiple processes on a single machine and makes it easily scalable to multiple machine within the same framework.

A basic dask cluster consists of a scheduler and a number of workers.

Scheduler recieves a task graph generated by APIs, which is a flow chart of what tasks are available, and send tasks to workers. 

Works will then work on their individual tasks.

This 

Let's start with creating a dask client

```{code-cell} ipython
from dask.distributed import Client, LocalCluster
client = Client()
```

By default, this is equivalent to setting up a local cluster 

```{code-cell} ipython
client.shutdown() # Shutdown our previous client

cluster = LocalCluster()
client = Client(cluster)
```

We can see that the client will store basic information about the cluster

```{code-cell} ipython
client
```

At client level, you can find a link to view the dashboard.

When you run this line on your own machine, you can open the link to see the the status of the cluster(s) and workers in the cluster(s).

Moving into **Cluster Info** tag, you can find basci information abou the cluster. Our machine is ... 

`Using Processes` is set to be true for use to run the cluster through multi-processing. 

We can see that processes are grouped into ... workers. Each worker will have its memory of size ...
