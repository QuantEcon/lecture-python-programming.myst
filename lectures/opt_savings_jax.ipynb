{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95f0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load opt_savings_jax.py\n",
    "# # Optimal savings with JAX\n",
    "#\n",
    "# #### Prepared for the CBC QuantEcon Workshop (September 2022)\n",
    "#\n",
    "# #### John Stachurski\n",
    "#\n",
    "# Re-implements the optimal savings problem using JAX.\n",
    "\n",
    "import numpy as np\n",
    "import quantecon as qe\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use 64 bit floats with JAX in order to match NumPy/Numba code\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "def successive_approx(T,                     # Operator (callable)\n",
    "                      x_0,                   # Initial condition\n",
    "                      tolerance=1e-6,        # Error tolerance\n",
    "                      max_iter=10_000,       # Max iteration bound\n",
    "                      print_step=25,         # Print at multiples\n",
    "                      verbose=False):        \n",
    "    x = x_0\n",
    "    error = tolerance + 1\n",
    "    k = 1\n",
    "    while error > tolerance and k <= max_iter:\n",
    "        x_new = T(x)\n",
    "        error = np.max(np.abs(x_new - x))\n",
    "        if verbose and k % print_step == 0:\n",
    "            print(f\"Completed iteration {k} with error {error}.\")\n",
    "        x = x_new\n",
    "        k += 1\n",
    "    if error > tolerance:\n",
    "        print(f\"Warning: Iteration hit upper bound {max_iter}.\")\n",
    "    elif verbose:\n",
    "        print(f\"Terminated successfully in {k} iterations.\")\n",
    "    return x\n",
    "\n",
    "\n",
    "# -\n",
    "\n",
    "# ##  Primitives and Operators \n",
    "\n",
    "# A namedtuple definition for storing parameters and grids\n",
    "Model = namedtuple('Model', \n",
    "                    ('β', 'R', 'γ', 'w_grid', 'y_grid', 'Q'))\n",
    "\n",
    "def create_consumption_model(R=1.01,                    # Gross interest rate\n",
    "                             β=0.98,                    # Discount factor\n",
    "                             γ=2.5,                     # CRRA parameter\n",
    "                             w_min=0.01,                # Min wealth\n",
    "                             w_max=5.0,                 # Max wealth\n",
    "                             w_size=150,                # Grid side\n",
    "                             ρ=0.9, ν=0.1, y_size=100): # Income parameters\n",
    "    \"\"\"\n",
    "    A function that takes in parameters and returns an instance of Model that\n",
    "    contains data for the optimal savings problem.\n",
    "    \"\"\"\n",
    "    w_grid = np.linspace(w_min, w_max, w_size)  \n",
    "    mc = qe.tauchen(ρ, ν, n=y_size)\n",
    "    y_grid, Q = np.exp(mc.state_values), mc.P\n",
    "    return Model(β=β, R=R, γ=γ, w_grid=w_grid, y_grid=y_grid, Q=Q)\n",
    "\n",
    "\n",
    "def create_consumption_model_jax():\n",
    "    \"Build a JAX-compatible version of the consumption model.\"\n",
    "\n",
    "    model = create_consumption_model()\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "\n",
    "    # Break up parameters into static and nonstatic components\n",
    "    constants = β, R, γ\n",
    "    sizes = len(w_grid), len(y_grid)\n",
    "    arrays = w_grid, y_grid, Q\n",
    "\n",
    "    # Shift arrays to the device (e.g., GPU)\n",
    "    arrays = tuple(map(jax.device_put, arrays))\n",
    "    return constants, sizes, arrays\n",
    "\n",
    "\n",
    "def B(v, constants, sizes, arrays):\n",
    "    \"\"\"\n",
    "    A vectorized version of the right-hand side of the Bellman equation \n",
    "    (before maximization), which is a 3D array representing\n",
    "\n",
    "        B(w, y, w′) = u(Rw + y - w′) + β Σ_y′ v(w′, y′) Q(y, y′)\n",
    "\n",
    "    for all (w, y, w′).\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack \n",
    "    β, R, γ = constants\n",
    "    w_size, y_size = sizes\n",
    "    w_grid, y_grid, Q = arrays\n",
    "\n",
    "    # Compute current rewards r(w, y, wp) as array r[i, j, ip]\n",
    "    w  = jnp.reshape(w_grid, (w_size, 1, 1))    # w[i]   ->  w[i, j, ip]\n",
    "    y  = jnp.reshape(y_grid, (1, y_size, 1))    # z[j]   ->  z[i, j, ip]\n",
    "    wp = jnp.reshape(w_grid, (1, 1, w_size))    # wp[ip] -> wp[i, j, ip]\n",
    "    c = R * w + y - wp\n",
    "\n",
    "    # Calculate continuation rewards at all combinations of (w, y, wp)\n",
    "    v = jnp.reshape(v, (1, 1, w_size, y_size))  # v[ip, jp] -> v[i, j, ip, jp]\n",
    "    Q = jnp.reshape(Q, (1, y_size, 1, y_size))  # Q[j, jp]  -> Q[i, j, ip, jp]\n",
    "    EV = jnp.sum(v * Q, axis=3)                 # sum over last index jp\n",
    "\n",
    "    # Compute the right-hand side of the Bellman equation\n",
    "    return jnp.where(c > 0, c**(1-γ)/(1-γ) + β * EV, -np.inf)\n",
    "\n",
    "\n",
    "def compute_r_σ(σ, constants, sizes, arrays):\n",
    "    \"\"\"\n",
    "    Compute the array r_σ[i, j] = r[i, j, σ[i, j]], which gives current\n",
    "    rewards given policy σ.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack model\n",
    "    β, R, γ = constants\n",
    "    w_size, y_size = sizes\n",
    "    w_grid, y_grid, Q = arrays\n",
    "\n",
    "    # Compute r_σ[i, j]\n",
    "    w = jnp.reshape(w_grid, (w_size, 1))\n",
    "    y = jnp.reshape(y_grid, (1, y_size))\n",
    "    wp = w_grid[σ]\n",
    "    c = R * w + y - wp\n",
    "    r_σ = c**(1-γ)/(1-γ)\n",
    "\n",
    "    return r_σ\n",
    "\n",
    "\n",
    "def T(v, constants, sizes, arrays):\n",
    "    \"The Bellman operator.\"\n",
    "    return jnp.max(B(v, constants, sizes, arrays), axis=2)\n",
    "\n",
    "\n",
    "def get_greedy(v, constants, sizes, arrays):\n",
    "    \"Computes a v-greedy policy, returned as a set of indices.\"\n",
    "    return jnp.argmax(B(v, constants, sizes, arrays), axis=2)\n",
    "\n",
    "\n",
    "def T_σ(v, σ, constants, sizes, arrays):\n",
    "    \"The σ-policy operator.\"\n",
    "\n",
    "    # Unpack model\n",
    "    β, R, γ = constants\n",
    "    w_size, y_size = sizes\n",
    "    w_grid, y_grid, Q = arrays\n",
    "\n",
    "    r_σ = compute_r_σ(σ, constants, sizes, arrays)\n",
    "\n",
    "    # Compute the array v[σ[i, j], jp]\n",
    "    yp_idx = jnp.arange(y_size)\n",
    "    yp_idx = jnp.reshape(yp_idx, (1, 1, y_size))\n",
    "    σ = jnp.reshape(σ, (w_size, y_size, 1))\n",
    "    V = v[σ, yp_idx]      \n",
    "\n",
    "    # Convert Q[j, jp] to Q[i, j, jp] \n",
    "    Q = jnp.reshape(Q, (1, y_size, y_size))\n",
    "\n",
    "    # Calculate the expected sum Σ_jp v[σ[i, j], jp] * Q[i, j, jp]\n",
    "    Ev = np.sum(V * Q, axis=2)\n",
    "\n",
    "    return r_σ + β * np.sum(V * Q, axis=2)\n",
    "\n",
    "\n",
    "def R_σ(v, σ, constants, sizes, arrays):\n",
    "    \"\"\"\n",
    "    The value v_σ of a policy σ is defined as \n",
    "\n",
    "        v_σ = (I - β P_σ)^{-1} r_σ\n",
    "\n",
    "    Here we set up the linear map v -> R_σ v, where R_σ := I - β P_σ. \n",
    "\n",
    "    In the consumption problem, this map can be expressed as\n",
    "\n",
    "        (R_σ v)(w, y) = v(w, y) - β Σ_y′ v(σ(w, y), y′) Q(y, y′)\n",
    "\n",
    "    Defining the map as above works in a more intuitive multi-index setting\n",
    "    (e.g. working with v[i, j] rather than flattening v to a one-dimensional\n",
    "    array) and avoids instantiating the large matrix P_σ.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    β, R, γ = constants\n",
    "    w_size, y_size = sizes\n",
    "    w_grid, y_grid, Q = arrays\n",
    "\n",
    "    # Set up the array v[σ[i, j], jp]\n",
    "    zp_idx = jnp.arange(y_size)\n",
    "    zp_idx = jnp.reshape(zp_idx, (1, 1, y_size))\n",
    "    σ = jnp.reshape(σ, (w_size, y_size, 1))\n",
    "    V = v[σ, zp_idx]\n",
    "\n",
    "    # Expand Q[j, jp] to Q[i, j, jp]\n",
    "    Q = jnp.reshape(Q, (1, y_size, y_size))\n",
    "\n",
    "    # Compute and return v[i, j] - β Σ_jp v[σ[i, j], jp] * Q[j, jp]\n",
    "    return v - β * np.sum(V * Q, axis=2)\n",
    "\n",
    "\n",
    "def get_value(σ, constants, sizes, arrays):\n",
    "    \"Get the value v_σ of policy σ by inverting the linear map R_σ.\"\n",
    "\n",
    "    # Unpack \n",
    "    β, R, γ = constants\n",
    "    w_size, y_size = sizes\n",
    "    w_grid, y_grid, Q = arrays\n",
    "\n",
    "    r_σ = compute_r_σ(σ, constants, sizes, arrays)\n",
    "\n",
    "    # Reduce R_σ to a function in v\n",
    "    partial_R_σ = lambda v: R_σ(v, σ, constants, sizes, arrays)\n",
    "\n",
    "    return jax.scipy.sparse.linalg.bicgstab(partial_R_σ, r_σ)[0]\n",
    "\n",
    "\n",
    "# ## JIT compiled versions\n",
    "\n",
    "B = jax.jit(B, static_argnums=(2,))\n",
    "compute_r_σ = jax.jit(compute_r_σ, static_argnums=(2,))\n",
    "T = jax.jit(T, static_argnums=(2,))\n",
    "get_greedy = jax.jit(get_greedy, static_argnums=(2,))\n",
    "\n",
    "get_value = jax.jit(get_value, static_argnums=(2,))\n",
    "\n",
    "T_σ = jax.jit(T_σ, static_argnums=(3,))\n",
    "R_σ = jax.jit(R_σ, static_argnums=(3,))\n",
    "\n",
    "\n",
    "# ##  Solvers\n",
    "\n",
    "def value_iteration(model, tol=1e-5):\n",
    "    \"Implements VFI.\"\n",
    "\n",
    "    constants, sizes, arrays = model\n",
    "    _T = lambda v: T(v, constants, sizes, arrays)\n",
    "    vz = jnp.zeros(sizes)\n",
    "\n",
    "    v_star = successive_approx(_T, vz, tolerance=tol)\n",
    "    return get_greedy(v_star, constants, sizes, arrays)\n",
    "\n",
    "def policy_iteration(model):\n",
    "    \"Howard policy iteration routine.\"\n",
    "    constants, sizes, arrays = model\n",
    "    vz = jnp.zeros(sizes)\n",
    "    σ = jnp.zeros(sizes, dtype=int)\n",
    "    i, error = 0, 1.0\n",
    "    while error > 0:\n",
    "        v_σ = get_value(σ, constants, sizes, arrays)\n",
    "        σ_new = get_greedy(v_σ, constants, sizes, arrays)\n",
    "        error = jnp.max(np.abs(σ_new - σ))\n",
    "        σ = σ_new\n",
    "        i = i + 1\n",
    "        print(f\"Concluded loop {i} with error {error}.\")\n",
    "    return σ\n",
    "\n",
    "def optimistic_policy_iteration(model, tol=1e-5, m=10):\n",
    "    \"Implements the OPI routine.\"\n",
    "    constants, sizes, arrays = model\n",
    "    v = jnp.zeros(sizes)\n",
    "    error = tol + 1\n",
    "    while error > tol:\n",
    "        last_v = v\n",
    "        σ = get_greedy(v, constants, sizes, arrays)\n",
    "        for _ in range(m):\n",
    "            v = T_σ(v, σ, constants, sizes, arrays)\n",
    "        error = jnp.max(np.abs(v - last_v))\n",
    "    return get_greedy(v, constants, sizes, arrays)\n",
    "\n",
    "\n",
    "# ## Plots\n",
    "\n",
    "fontsize=12\n",
    "model = create_consumption_model_jax()\n",
    "# Unpack \n",
    "constants, sizes, arrays = model\n",
    "β, R, γ = constants\n",
    "w_size, y_size = sizes\n",
    "w_grid, y_grid, Q = arrays\n",
    "σ_star = policy_iteration(model)\n",
    "fig, ax = plt.subplots(figsize=(9, 5.2))\n",
    "ax.plot(w_grid, w_grid, \"k--\", label=\"45\")\n",
    "ax.plot(w_grid, w_grid[σ_star[:, 1]], label=\"$\\\\sigma^*(\\cdot, y_1)$\")\n",
    "ax.plot(w_grid, w_grid[σ_star[:, -1]], label=\"$\\\\sigma^*(\\cdot, y_N)$\")\n",
    "ax.legend(fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "# ## Tests\n",
    "\n",
    "model = create_consumption_model_jax()\n",
    "\n",
    "print(\"Starting HPI.\")\n",
    "qe.tic()\n",
    "out = policy_iteration(model)\n",
    "elapsed = qe.toc()\n",
    "print(out)\n",
    "print(f\"HPI completed in {elapsed} seconds.\")\n",
    "\n",
    "print(\"Starting VFI.\")\n",
    "qe.tic()\n",
    "out = value_iteration(model)\n",
    "elapsed = qe.toc()\n",
    "print(out)\n",
    "print(f\"VFI completed in {elapsed} seconds.\")\n",
    "\n",
    "print(\"Starting OPI.\")\n",
    "qe.tic()\n",
    "out = optimistic_policy_iteration(model, m=100)\n",
    "elapsed = qe.toc()\n",
    "print(out)\n",
    "print(f\"OPI completed in {elapsed} seconds.\")\n",
    "\n",
    "\n",
    "# +\n",
    "m_vals = range(5, 3000, 100)\n",
    "model = create_consumption_model_jax()\n",
    "print(\"Running Howard policy iteration.\")\n",
    "qe.tic()\n",
    "σ_pi = policy_iteration(model)\n",
    "pi_time = qe.toc()\n",
    "print(f\"PI completed in {pi_time} seconds.\")\n",
    "\n",
    "print(\"Running value function iteration.\")\n",
    "qe.tic()\n",
    "σ_vfi = value_iteration(model, tol=1e-5)\n",
    "vfi_time = qe.toc()\n",
    "print(f\"VFI completed in {vfi_time} seconds.\")\n",
    "\n",
    "opi_times = []\n",
    "for m in m_vals:\n",
    "    qe.tic()\n",
    "    σ_opi = optimistic_policy_iteration(model, m=m, tol=1e-5)\n",
    "    opi_time = qe.toc()\n",
    "    print(f\"OPI with m={m} completed in {opi_time} seconds.\")\n",
    "    opi_times.append(opi_time)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(9, 5.2))\n",
    "ax.plot(m_vals, np.full(len(m_vals), pi_time), \n",
    "        lw=2, label=\"Howard policy iteration\")\n",
    "ax.plot(m_vals, np.full(len(m_vals), vfi_time), \n",
    "        lw=2, label=\"value function iteration\")\n",
    "ax.plot(m_vals, opi_times, lw=2, label=\"optimistic policy iteration\")\n",
    "ax.legend(fontsize=fontsize, frameon=False)\n",
    "ax.set_xlabel(\"$m$\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"time\", fontsize=fontsize)\n",
    "plt.show()\n",
    "# -\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4537d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
