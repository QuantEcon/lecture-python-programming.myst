---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

(dask)=
```{raw} jupyter
<div id="qe-notebook-header" align="right" style="text-align:right;">
        <a href="https://quantecon.org/" title="quantecon.org">
                <img style="width:250px;display:inline;" width="250px" src="https://assets.quantecon.org/img/qe-menubar-logo.svg" alt="QuantEcon">
        </a>
</div>
```


# Dask

## Overview

In this lecture, we will continue our exploration on parallelization in Python.

Modern datasets are in the size of trillion, which requires more computing power to speed operations on enomours datasets.

We would want to harvest the processing power in our local machines and potentially clusters in data processing.

We mentioned that multiprocessing is different from multithreading their treatment of memory space. 

In multiprocessing, each process has its own memory space and communicate data through channels between processes.

The overheads and communication cost is higher for multiprocessing compared to multithreading, which is lightweight and easier to start.

But, it brings several desirable advantages:

- Multiprocessing is another way to bypass GIL by employing subprocesses instead of threads.
- Multiprocessing is more flexible and can be distributed across clusters.
- It can speed up operations that are CPU-bound, which cannot be accelerated using multithreading.

Dask is a popular Python library that allows users to speed up operatios conveniently in high-level implementations and control the low level APIs such as Delayed and Future to parallelize customise algorithms.

This lecture will focus on [Dask](https://docs.dask.org/en/stable/why.html#why-dask) because

- It builds on the familiar command in NumPy and Pandas.
- It can be deployed on a single machine and easily scaled to multiple clusters.
- It will carefully manage the memory to enable operations that are larger than the memory of the machine.

In this lecture, we will explore benefits that Dask brings. 

We will start with the general framework, go through high-level APIs of Dask, and then dig into the lower level APIs.

## Dask Clusters

Dask clusters gives Dask the power to leverage the power of multiple processes on a single machine and makes it easily scalable to multiple machine within the same framework.

A basic dask cluster consists of a scheduler and a number of workers.

Scheduler recieves a task graph generated by APIs, which is a flow chart of what tasks are available, and send tasks to workers. 

Works will then work on their individual tasks.

Let's start with creating a dask client

```{code-cell} ipython
from dask.distributed import Client, LocalCluster
client = Client()
```

By default, this is equivalent to setting up a local cluster 

```{code-cell} ipython
client.shutdown() # Shutdown our previous client

cluster = LocalCluster()
client = Client(cluster)
```

We can see that the client will store basic information about the cluster

```{code-cell} ipython
client
```

At client level, you can find a link to view the dashboard.

When you run this line on your own machine, you can open the link to see the the status of the cluster(s) and workers in the cluster(s).

Moving into **Cluster Info** tag, you can find basci information abou the cluster. Our machine is ... 

`Using Processes` is set to be true for use to run the cluster through multi-processing. 

We can see that processes are grouped into ... workers. Each worker will have its memory of size ...


## Dask Array and DataFrame

Dask Array and DataFrame are very powerful tools when dealing with performance bounded tasks and array and dataframes that are larger than the memory of the machine.

The main purpose of Dask is to make the operations scalable to very large tasks while maintaining a good perforamce.

When handling tasks that are smaller in scale, it is often recommended to use speedup tools such as [JAX](https://python-programming.quantecon.org/jax_intro.html#jax-as-a-numpy-replacement) and [Numba](https://python-programming.quantecon.org/numba.html).

### Dask Array
Dask array implements some of the NumPy Array inferface using paralleled programing. 

The idea is to reduce a large array operations into operations on smaller blocks and parallel these operations
on smaller blocks.

![daArray](lectures/_static/lecture_specific/dask/da.png)

This also gives Dask power to work with arrays that are larger than the memory of machine.

The syntax for Dask array is very similar to NumPy.


```{code-cell} ipython
import dask.array as da

x = da.random.standard_normal((10000, 10000))
x
```

```{code-cell} ipython
std_x = da.std(x, axis=1)
std_x
```

However, instead of calculating the result directly, Dask prints out a summary of the output array.

It is one of the Dask Collections.

```{code-cell} ipython
type(std_x)
```

This is because Dask is carefully scheduling the operations on each array subset to optimize the execution of code.

We can visualize the task graph to see the steps that Dask takes to reach the final output

```{code-cell} ipython
std_x.visualize()
```

In the task graph, Dask generates 16 small arrays to form a large array with dimension `(10000, 10000)` provided by us.

Then, it uses four subsets of the larger array to calculate the standard deviation of each row.

Now, we can call `.compute()` to generate the result.

```{code-cell} ipython
mean_x.compute()
```

We can further calculate the mean of the standard devaitions
```{code-cell} ipython
std_mean = da.mean(std_x)
```

The new operation will be added to the task graph

```{code-cell} ipython
std_mean.visualize()
```

Let's compute the final result

```{code-cell} ipython
std_mean.compute()
```

```{note}
It is recommended to avoid the excessive use of `compute()`; only call `compute()` when the final result to avoid repetitive computations.
```

We can assess how workers perform using the Dashboard link.

If you are running on Google Colab, you can generate downloadable performance report using

```{code-cell} ipython
from dask.distributed import performance_report

with performance_report(filename="dask-report.html"):
  da.mean(std_x).compute()
```

### Dask DataFrame

The intuition behind Dask DataFrame is very similar to Dask Array. 

![DataFrame](lectures/_static/lecture_specific/dask/dd.png)

A large Dask DataFrame is partitioned into smaller DataFrames and each worker will only work with a small subset of the large DataFrame.




### Dask Futures

