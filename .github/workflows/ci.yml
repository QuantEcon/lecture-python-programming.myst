name: Build Project [using jupyter-book]
on: [pull_request]
jobs:
  preview:
    runs-on: "runs-on=${{ github.run_id }}/family=g4dn.2xlarge/image=quantecon_ubuntu2404/disk=large"
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0  # Fetch full git history for changelog feature
      - name: Setup Anaconda
        uses: conda-incubator/setup-miniconda@v3
        with:
          auto-update-conda: true
          auto-activate-base: true
          miniconda-version: 'latest'
          python-version: "3.13"
          environment-file: environment.yml
          activate-environment: quantecon
      - name: Check nvidia Drivers
        shell: bash -l {0}
        run: nvidia-smi
      - name: Install JAX and Numpyro
        shell: bash -l {0}
        run: |
          pip install -U "jax[cuda13]"
          pip install numpyro
          python scripts/test-jax-install.py
      # === lax.scan GPU Performance Profiling ===
      - name: Profile lax.scan (GPU vs CPU)
        shell: bash -l {0}
        run: |
          echo "=== lax.scan Performance Profiling ==="
          echo "This profiles the known issue with lax.scan on GPU (JAX Issue #2491)"
          echo ""
          python scripts/profile_lax_scan.py --iterations 100000
          echo ""
          echo "Note: GPU is expected to be much slower due to CPU-GPU sync per iteration"
      # === Benchmark Tests (Bare Metal, Jupyter, Jupyter-Book) ===
      - name: Run Hardware Benchmarks (Bare Metal)
        shell: bash -l {0}
        run: |
          echo "=== Bare Metal Python Script Execution ==="
          python scripts/benchmark-hardware.py
          mkdir -p benchmark_results
          mv benchmark_results_bare_metal.json benchmark_results/
      - name: Run Jupyter Notebook Benchmark (via nbconvert)
        shell: bash -l {0}
        run: |
          echo "=== Jupyter Kernel Execution ==="
          cd scripts
          jupyter nbconvert --to notebook --execute benchmark-jupyter.ipynb --output benchmark-jupyter-executed.ipynb
          echo "Notebook executed successfully"
          cd ..
          mv scripts/benchmark_results_jupyter.json benchmark_results/
      - name: Run Jupyter-Book Benchmark
        shell: bash -l {0}
        run: |
          echo "=== Jupyter-Book Execution ==="
          # Build just the benchmark file using jupyter-book
          mkdir -p benchmark_test
          cp scripts/benchmark-jupyterbook.md benchmark_test/
          # Create minimal _config.yml
          echo "title: Benchmark Test" > benchmark_test/_config.yml
          echo "execute:" >> benchmark_test/_config.yml
          echo "  execute_notebooks: force" >> benchmark_test/_config.yml
          # Create minimal _toc.yml
          echo "format: jb-book" > benchmark_test/_toc.yml
          echo "root: benchmark-jupyterbook" >> benchmark_test/_toc.yml
          # Build (run from benchmark_test so JSON is written there)
          cd benchmark_test
          jb build . --path-output ../benchmark_build/
          cd ..
          echo "Jupyter-Book build completed successfully"
          # Move JSON results if generated
          cp benchmark_test/benchmark_results_jupyterbook.json benchmark_results/ 2>/dev/null || echo "No jupyterbook results"
      - name: Collect and Display Benchmark Results
        shell: bash -l {0}
        run: |
          echo "=== Benchmark Results Summary ==="
          for f in benchmark_results/*.json; do
            echo "--- $f ---"
            cat "$f"
            echo ""
          done
      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v5
        with:
          name: benchmark-results
          path: benchmark_results/
          if-no-files-found: warn
      - name: Install latex dependencies
        run: |
          sudo apt-get -qq update
          sudo apt-get install -y     \
            texlive-latex-recommended \
            texlive-latex-extra       \
            texlive-fonts-recommended \
            texlive-fonts-extra       \
            texlive-xetex             \
            latexmk                   \
            xindy                     \
            dvipng                    \
            cm-super
      - name: Display Conda Environment Versions
        shell: bash -l {0}
        run: conda list
      - name: Display Pip Versions
        shell: bash -l {0}
        run: pip list
      - name: Download "build" folder (cache)
        uses: dawidd6/action-download-artifact@v11
        with:
          workflow: cache.yml
          branch: main
          name: build-cache
          path: _build
      # Build Assets (Download Notebooks and PDF via LaTeX)
      - name: Build Download Notebooks (sphinx-tojupyter)
        shell: bash -l {0}
        run: |
          jb build lectures --path-output ./ --builder=custom --custom-builder=jupyter -n -W --keep-going
          mkdir -p _build/html/_notebooks
          cp -u _build/jupyter/*.ipynb _build/html/_notebooks
      - name: Build PDF from LaTeX
        shell: bash -l {0}
        run: |
          jb build lectures --builder pdflatex --path-output ./ -n -W --keep-going
          mkdir _build/html/_pdf
          cp -u _build/latex/*.pdf _build/html/_pdf
      # Final Build of HTML
      - name: Build HTML
        shell: bash -l {0}
        run: |
          jb build lectures --path-output ./ -n -W --keep-going
      - name: Upload Execution Reports
        uses: actions/upload-artifact@v5
        if: failure()
        with:
          name: execution-reports
          path: _build/html/reports
      - name: Preview Deploy to Netlify
        uses: nwtgck/actions-netlify@v3
        with:
          publish-dir: '_build/html/'
          production-branch: main
          github-token: ${{ secrets.GITHUB_TOKEN }}
          deploy-message: "Preview Deploy from GitHub Actions"
        env:
          NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
          NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}
